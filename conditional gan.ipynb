{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_chan = 1, hidden_dim = 64, labels = 0):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.disc_block(im_chan+labels, hidden_dim, 4, 2, 1), # 32x32 \n",
    "            self.disc_block(hidden_dim, hidden_dim*2, 4,2,1), # 16x 16 \n",
    "            self.disc_block(hidden_dim*2, hidden_dim*4, 4,2,1), # 8x8 \n",
    "            self.disc_block(hidden_dim*4, hidden_dim*8, 4,2,1), # 4x4 \n",
    "            self.disc_block(hidden_dim*8, 1, 4,2,0, True), # 1x1 \n",
    "        )\n",
    "        \n",
    "    def disc_block(self, input_channels, output_channels, kernel_size= 4, stride = 2,padding = 0 , final_layer = False):\n",
    "        if not final_layer: \n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride,padding = padding, bias = False),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace = True)\n",
    "            )\n",
    "        else: \n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        disc_pred = self.disc(x)\n",
    "        return disc_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 100, hidden_dim = 32, img_channel=1, labels = 0):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = z_dim + labels \n",
    "        output_dim = img_channel\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(input_dim , hidden_dim*8, 4, 2, 0),  # 4x4 \n",
    "            self.gen_block(hidden_dim*8,hidden_dim*4,  4, 2, 1), # 8x8 \n",
    "            self.gen_block(hidden_dim*4,hidden_dim*2,  4, 2, 1), # 16x 16 \n",
    "            self.gen_block(hidden_dim*2,hidden_dim,  4, 2, 1), # 32 x 32 \n",
    "            self.gen_block(hidden_dim, output_dim, 4, 2, 1, True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        )\n",
    "        \n",
    "    def gen_block(self,input_dim,output_dim, kernel_size = 4, stride = 1, padding = 0, final_layer = False):\n",
    "        if not final_layer: \n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_dim,output_dim,kernel_size, stride, padding, bias = False),\n",
    "                nn.BatchNorm2d(output_dim),\n",
    "                nn.LeakyReLU(0.2, inplace = True)\n",
    "            )\n",
    "        else: \n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_dim,output_dim, kernel_size, stride, padding),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.gen(x)\n",
    "        return out \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100 \n",
    "N_classes = 10\n",
    "nlabels = 10 \n",
    "img_size = 64\n",
    "img_channel = 1\n",
    "epochs = 200\n",
    "batch_size = 128 \n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST('.', download = True, transform = transform)\n",
    "loader = DataLoader(dataset, \n",
    "                   batch_size = batch_size, \n",
    "                   shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 100, hidden_dim = 32, img_channel= img_channel, labels = 10).to(device)\n",
    "disc = Discriminator(im_chan = img_channel, hidden_dim = 64, labels = 10).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr = 0.0001, betas=(0, 0.9))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr = 0.0001, betas = (0, 0.9))\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (disc): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(11, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tensorboard plotting\n",
    "noise = torch.randn(32, z_dim,1,1).to(device)\n",
    "y_labels = torch.randint(0,9, (32,)).to(device)\n",
    "labels_onehot = F.one_hot(y_labels, nlabels).view(32, -1, 1,1).to(device)\n",
    "noise_embed_label = torch.cat([noise, labels_onehot], axis = 1)\n",
    "\n",
    "fixed_noise = noise_embed_label \n",
    "writer_real = SummaryWriter(f\"logs/GAN_MNIST4/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/GAN_MNIST4/fake\")\n",
    "CRITIC_ITERATIONS = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake,img_labels, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "    \n",
    "\n",
    "    # Calculate critic scores\n",
    "    \n",
    "    int_img_nd_labels = torch.cat([interpolated_images, img_labels], axis =1)\n",
    "    mixed_scores = critic(int_img_nd_labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=int_img_nd_labels,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Batch 20/469                   Loss D: -4.9631, loss G: 2.7132\n",
      "Epoch [0/200] Batch 40/469                   Loss D: -12.7823, loss G: 6.4920\n",
      "Epoch [0/200] Batch 60/469                   Loss D: -23.8688, loss G: 11.8114\n",
      "Epoch [0/200] Batch 80/469                   Loss D: -38.1554, loss G: 18.5943\n",
      "Epoch [0/200] Batch 100/469                   Loss D: -54.9208, loss G: 26.8070\n",
      "Epoch [0/200] Batch 120/469                   Loss D: -75.6240, loss G: 36.5842\n",
      "Epoch [0/200] Batch 140/469                   Loss D: -99.2053, loss G: 47.8824\n",
      "Epoch [0/200] Batch 160/469                   Loss D: -125.7252, loss G: 60.5838\n",
      "Epoch [0/200] Batch 180/469                   Loss D: -155.6257, loss G: 74.8743\n",
      "Epoch [0/200] Batch 200/469                   Loss D: -188.4962, loss G: 90.4659\n",
      "Epoch [0/200] Batch 220/469                   Loss D: -224.4855, loss G: 107.7368\n",
      "Epoch [0/200] Batch 240/469                   Loss D: -263.8954, loss G: 126.4942\n",
      "Epoch [0/200] Batch 260/469                   Loss D: -305.9883, loss G: 146.5115\n",
      "Epoch [0/200] Batch 280/469                   Loss D: -351.5845, loss G: 168.1221\n",
      "Epoch [0/200] Batch 300/469                   Loss D: -400.6237, loss G: 191.4981\n",
      "Epoch [0/200] Batch 320/469                   Loss D: -452.4776, loss G: 216.1970\n",
      "Epoch [0/200] Batch 340/469                   Loss D: -506.9857, loss G: 242.0605\n",
      "Epoch [0/200] Batch 360/469                   Loss D: -564.7662, loss G: 269.3889\n",
      "Epoch [0/200] Batch 380/469                   Loss D: -625.4413, loss G: 297.5923\n",
      "Epoch [0/200] Batch 400/469                   Loss D: -690.8045, loss G: 329.3765\n",
      "Epoch [0/200] Batch 420/469                   Loss D: -758.3998, loss G: 361.3756\n",
      "Epoch [0/200] Batch 440/469                   Loss D: -828.2067, loss G: 394.2569\n",
      "Epoch [0/200] Batch 460/469                   Loss D: -902.4982, loss G: 429.7130\n",
      "Epoch [1/200] Batch 20/469                   Loss D: -1013.7274, loss G: 482.9300\n",
      "Epoch [1/200] Batch 40/469                   Loss D: -1095.9386, loss G: 521.6851\n",
      "Epoch [1/200] Batch 60/469                   Loss D: -1180.7621, loss G: 561.7366\n",
      "Epoch [1/200] Batch 80/469                   Loss D: -1268.3180, loss G: 603.2042\n",
      "Epoch [1/200] Batch 100/469                   Loss D: -1358.7120, loss G: 645.7762\n",
      "Epoch [1/200] Batch 120/469                   Loss D: -1451.5374, loss G: 690.1103\n",
      "Epoch [1/200] Batch 140/469                   Loss D: -1549.6422, loss G: 736.4869\n",
      "Epoch [1/200] Batch 160/469                   Loss D: -1650.0126, loss G: 783.9774\n",
      "Epoch [1/200] Batch 180/469                   Loss D: -1753.0977, loss G: 832.7686\n",
      "Epoch [1/200] Batch 200/469                   Loss D: -1859.2194, loss G: 883.1046\n",
      "Epoch [1/200] Batch 220/469                   Loss D: -1968.5934, loss G: 934.6743\n",
      "Epoch [1/200] Batch 240/469                   Loss D: -2081.1509, loss G: 988.2037\n",
      "Epoch [1/200] Batch 260/469                   Loss D: -2121.9487, loss G: 973.8542\n",
      "Epoch [1/200] Batch 280/469                   Loss D: -2309.7666, loss G: 1094.3885\n",
      "Epoch [1/200] Batch 300/469                   Loss D: -2435.1455, loss G: 1156.2559\n",
      "Epoch [1/200] Batch 320/469                   Loss D: -2558.2434, loss G: 1214.4408\n",
      "Epoch [1/200] Batch 340/469                   Loss D: -2681.5703, loss G: 1273.2695\n",
      "Epoch [1/200] Batch 360/469                   Loss D: -2816.4473, loss G: 1336.8295\n",
      "Epoch [1/200] Batch 380/469                   Loss D: -2937.8445, loss G: 1395.9988\n",
      "Epoch [1/200] Batch 400/469                   Loss D: -3053.4243, loss G: 1445.9481\n",
      "Epoch [1/200] Batch 420/469                   Loss D: -3198.4795, loss G: 1519.1261\n",
      "Epoch [1/200] Batch 440/469                   Loss D: -3344.9392, loss G: 1587.2957\n",
      "Epoch [1/200] Batch 460/469                   Loss D: -3480.6143, loss G: 1652.8960\n",
      "Epoch [2/200] Batch 20/469                   Loss D: -3613.9856, loss G: 1706.6993\n",
      "Epoch [2/200] Batch 40/469                   Loss D: -3797.9902, loss G: 1802.2500\n",
      "Epoch [2/200] Batch 60/469                   Loss D: -3954.0850, loss G: 1877.8147\n",
      "Epoch [2/200] Batch 80/469                   Loss D: -4096.9341, loss G: 1947.5161\n",
      "Epoch [2/200] Batch 100/469                   Loss D: -4260.3154, loss G: 2024.6199\n",
      "Epoch [2/200] Batch 120/469                   Loss D: -4409.1846, loss G: 2094.4004\n",
      "Epoch [2/200] Batch 140/469                   Loss D: -4493.2681, loss G: 2121.2517\n",
      "Epoch [2/200] Batch 160/469                   Loss D: -4729.0630, loss G: 2246.2495\n",
      "Epoch [2/200] Batch 180/469                   Loss D: -4826.0571, loss G: 1469.0488\n",
      "Epoch [2/200] Batch 200/469                   Loss D: -5064.8994, loss G: 2405.5864\n",
      "Epoch [2/200] Batch 220/469                   Loss D: -5248.8325, loss G: 2492.7295\n",
      "Epoch [2/200] Batch 240/469                   Loss D: -5251.5093, loss G: 2567.4233\n",
      "Epoch [2/200] Batch 260/469                   Loss D: -5611.0996, loss G: 2666.6804\n",
      "Epoch [2/200] Batch 280/469                   Loss D: -5608.8091, loss G: 2647.2637\n",
      "Epoch [2/200] Batch 300/469                   Loss D: -5966.2671, loss G: 2835.2507\n",
      "Epoch [2/200] Batch 320/469                   Loss D: -4127.4053, loss G: 2478.6304\n",
      "Epoch [2/200] Batch 340/469                   Loss D: -6326.7324, loss G: 3010.5000\n",
      "Epoch [2/200] Batch 360/469                   Loss D: -6546.1704, loss G: 3107.8296\n",
      "Epoch [2/200] Batch 380/469                   Loss D: -6352.0664, loss G: 3163.9805\n",
      "Epoch [2/200] Batch 400/469                   Loss D: -6949.7090, loss G: 3300.9746\n",
      "Epoch [2/200] Batch 420/469                   Loss D: -7164.5981, loss G: 3404.9102\n",
      "Epoch [2/200] Batch 440/469                   Loss D: -7373.6826, loss G: 3505.1885\n",
      "Epoch [2/200] Batch 460/469                   Loss D: -7588.9771, loss G: 3606.1489\n",
      "Epoch [3/200] Batch 20/469                   Loss D: -7890.2578, loss G: 3755.0679\n",
      "Epoch [3/200] Batch 40/469                   Loss D: -8123.8271, loss G: 3862.8486\n",
      "Epoch [3/200] Batch 60/469                   Loss D: -8100.8711, loss G: 3951.4148\n",
      "Epoch [3/200] Batch 80/469                   Loss D: -8567.2793, loss G: 4070.5383\n",
      "Epoch [3/200] Batch 100/469                   Loss D: -8799.7041, loss G: 4183.2974\n",
      "Epoch [3/200] Batch 120/469                   Loss D: -9019.7998, loss G: 4292.7695\n",
      "Epoch [3/200] Batch 140/469                   Loss D: -9069.3975, loss G: 4366.3491\n",
      "Epoch [3/200] Batch 160/469                   Loss D: -9319.6055, loss G: 4507.0439\n",
      "Epoch [3/200] Batch 180/469                   Loss D: -9766.2109, loss G: 4641.8291\n",
      "Epoch [3/200] Batch 200/469                   Loss D: -9980.8613, loss G: 4742.0044\n",
      "Epoch [3/200] Batch 220/469                   Loss D: -9419.5859, loss G: 4836.8906\n",
      "Epoch [3/200] Batch 240/469                   Loss D: -10497.9824, loss G: 4991.4238\n",
      "Epoch [3/200] Batch 260/469                   Loss D: -10751.9609, loss G: 5111.2705\n",
      "Epoch [3/200] Batch 280/469                   Loss D: -10979.1689, loss G: 5234.1382\n",
      "Epoch [3/200] Batch 300/469                   Loss D: -11269.0186, loss G: 5357.6470\n",
      "Epoch [3/200] Batch 320/469                   Loss D: -11533.5391, loss G: 5485.3691\n",
      "Epoch [3/200] Batch 340/469                   Loss D: -11716.9746, loss G: 5585.6211\n",
      "Epoch [3/200] Batch 360/469                   Loss D: -11551.2324, loss G: 5674.5669\n",
      "Epoch [3/200] Batch 380/469                   Loss D: -11562.2539, loss G: 5813.3154\n",
      "Epoch [3/200] Batch 400/469                   Loss D: -12615.7656, loss G: 6002.2998\n",
      "Epoch [3/200] Batch 420/469                   Loss D: -12887.9551, loss G: 6131.6279\n",
      "Epoch [3/200] Batch 440/469                   Loss D: -13182.2979, loss G: 6271.2700\n",
      "Epoch [3/200] Batch 460/469                   Loss D: -13457.7109, loss G: 6403.8564\n",
      "Epoch [4/200] Batch 20/469                   Loss D: -13286.7656, loss G: 6578.1582\n",
      "Epoch [4/200] Batch 40/469                   Loss D: -14041.1611, loss G: 6688.2207\n",
      "Epoch [4/200] Batch 60/469                   Loss D: -14418.7451, loss G: 6869.3154\n",
      "Epoch [4/200] Batch 80/469                   Loss D: -14734.0693, loss G: 7015.9678\n",
      "Epoch [4/200] Batch 100/469                   Loss D: -15005.0352, loss G: 7143.9409\n",
      "Epoch [4/200] Batch 120/469                   Loss D: -13134.7080, loss G: 7110.0234\n",
      "Epoch [4/200] Batch 140/469                   Loss D: -15558.7773, loss G: 7412.8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/200] Batch 160/469                   Loss D: -15932.5830, loss G: 7589.7295\n",
      "Epoch [4/200] Batch 180/469                   Loss D: -16232.8428, loss G: 7734.5166\n",
      "Epoch [4/200] Batch 200/469                   Loss D: -16541.7070, loss G: 7881.7715\n",
      "Epoch [4/200] Batch 220/469                   Loss D: -16834.3652, loss G: 8024.9629\n",
      "Epoch [4/200] Batch 240/469                   Loss D: -17115.2812, loss G: 8173.7637\n",
      "Epoch [4/200] Batch 260/469                   Loss D: -17424.6543, loss G: 8303.2217\n",
      "Epoch [4/200] Batch 280/469                   Loss D: -17592.9492, loss G: 8363.1328\n",
      "Epoch [4/200] Batch 300/469                   Loss D: -17907.7656, loss G: 8538.8809\n",
      "Epoch [4/200] Batch 320/469                   Loss D: -18227.3301, loss G: 8681.7939\n",
      "Epoch [4/200] Batch 340/469                   Loss D: -18565.4121, loss G: 8859.2812\n",
      "Epoch [4/200] Batch 360/469                   Loss D: -18920.6934, loss G: 9025.5527\n",
      "Epoch [4/200] Batch 380/469                   Loss D: -19233.5000, loss G: 9179.0957\n",
      "Epoch [4/200] Batch 400/469                   Loss D: -19487.7070, loss G: 9310.3789\n",
      "Epoch [4/200] Batch 420/469                   Loss D: -19612.5234, loss G: 9406.8711\n",
      "Epoch [4/200] Batch 440/469                   Loss D: -20223.2070, loss G: 9654.1719\n",
      "Epoch [4/200] Batch 460/469                   Loss D: -20522.2969, loss G: 9800.7627\n",
      "Epoch [5/200] Batch 20/469                   Loss D: -21000.8457, loss G: 10029.7695\n",
      "Epoch [5/200] Batch 40/469                   Loss D: -21309.7031, loss G: 10173.6172\n",
      "Epoch [5/200] Batch 60/469                   Loss D: -21630.1855, loss G: 10340.9561\n",
      "Epoch [5/200] Batch 80/469                   Loss D: -21750.8105, loss G: 10469.2246\n",
      "Epoch [5/200] Batch 100/469                   Loss D: -22270.8066, loss G: 10649.9141\n",
      "Epoch [5/200] Batch 120/469                   Loss D: -22354.6582, loss G: 10628.2402\n",
      "Epoch [5/200] Batch 140/469                   Loss D: -22839.4180, loss G: 10942.3994\n",
      "Epoch [5/200] Batch 160/469                   Loss D: -20511.4473, loss G: 11029.1699\n",
      "Epoch [5/200] Batch 180/469                   Loss D: -23416.9238, loss G: 11205.2734\n",
      "Epoch [5/200] Batch 200/469                   Loss D: -23647.2324, loss G: 11321.1045\n",
      "Epoch [5/200] Batch 220/469                   Loss D: -23300.6016, loss G: 11053.9336\n",
      "Epoch [5/200] Batch 240/469                   Loss D: -22724.1484, loss G: 10765.6133\n",
      "Epoch [5/200] Batch 260/469                   Loss D: -24267.6367, loss G: 11614.8701\n",
      "Epoch [5/200] Batch 280/469                   Loss D: -23782.2891, loss G: 11236.6816\n",
      "Epoch [5/200] Batch 300/469                   Loss D: -24427.5664, loss G: 11716.0439\n",
      "Epoch [5/200] Batch 320/469                   Loss D: -23899.4961, loss G: 11444.1689\n",
      "Epoch [5/200] Batch 340/469                   Loss D: -25413.9902, loss G: 12193.2969\n",
      "Epoch [5/200] Batch 360/469                   Loss D: -25192.8438, loss G: 12003.2920\n",
      "Epoch [5/200] Batch 380/469                   Loss D: -24101.0234, loss G: 12199.8662\n",
      "Epoch [5/200] Batch 400/469                   Loss D: -24779.6738, loss G: 11711.4648\n",
      "Epoch [5/200] Batch 420/469                   Loss D: -3090.0574, loss G: -9353.6641\n",
      "Epoch [5/200] Batch 440/469                   Loss D: -24945.3340, loss G: 12328.5508\n",
      "Epoch [5/200] Batch 460/469                   Loss D: 452.3737, loss G: -13469.2559\n",
      "Epoch [6/200] Batch 20/469                   Loss D: -2608.0930, loss G: 2057.3774\n",
      "Epoch [6/200] Batch 40/469                   Loss D: 1767.1162, loss G: -13351.9902\n",
      "Epoch [6/200] Batch 60/469                   Loss D: -15823.6562, loss G: 9301.2559\n",
      "Epoch [6/200] Batch 80/469                   Loss D: -450.1841, loss G: -13042.8008\n",
      "Epoch [6/200] Batch 100/469                   Loss D: -404.2386, loss G: -12953.3125\n",
      "Epoch [6/200] Batch 120/469                   Loss D: -236.7521, loss G: -13243.9150\n",
      "Epoch [6/200] Batch 140/469                   Loss D: -483.3505, loss G: -13276.8906\n",
      "Epoch [6/200] Batch 160/469                   Loss D: -582.9252, loss G: -13079.0391\n",
      "Epoch [6/200] Batch 180/469                   Loss D: -437.2877, loss G: -12932.5879\n",
      "Epoch [6/200] Batch 200/469                   Loss D: -853.9315, loss G: -12726.0801\n",
      "Epoch [6/200] Batch 220/469                   Loss D: -992.0038, loss G: -12583.8682\n",
      "Epoch [6/200] Batch 240/469                   Loss D: -879.6588, loss G: -12426.5156\n",
      "Epoch [6/200] Batch 260/469                   Loss D: -644.6915, loss G: -12548.5352\n",
      "Epoch [6/200] Batch 280/469                   Loss D: -929.6198, loss G: -12404.0000\n",
      "Epoch [6/200] Batch 300/469                   Loss D: -975.6317, loss G: -12364.2773\n",
      "Epoch [6/200] Batch 320/469                   Loss D: -12.5053, loss G: -12337.2910\n",
      "Epoch [6/200] Batch 340/469                   Loss D: -6.7449, loss G: -12299.6143\n",
      "Epoch [6/200] Batch 360/469                   Loss D: -315.9417, loss G: -12317.1348\n",
      "Epoch [6/200] Batch 380/469                   Loss D: 15.2424, loss G: -12302.4521\n",
      "Epoch [6/200] Batch 400/469                   Loss D: -552.7991, loss G: -12330.8262\n",
      "Epoch [6/200] Batch 420/469                   Loss D: 815.3245, loss G: -12193.2559\n",
      "Epoch [6/200] Batch 440/469                   Loss D: -171.8302, loss G: -12212.8242\n",
      "Epoch [6/200] Batch 460/469                   Loss D: -612.7079, loss G: -12452.0840\n",
      "Epoch [7/200] Batch 20/469                   Loss D: -692.0963, loss G: -12440.3252\n",
      "Epoch [7/200] Batch 40/469                   Loss D: 287.3485, loss G: -12226.7617\n",
      "Epoch [7/200] Batch 60/469                   Loss D: -777.6979, loss G: -12204.3662\n",
      "Epoch [7/200] Batch 80/469                   Loss D: 301.7706, loss G: -12259.1172\n",
      "Epoch [7/200] Batch 100/469                   Loss D: -149.7133, loss G: -12214.0928\n",
      "Epoch [7/200] Batch 120/469                   Loss D: 89.9841, loss G: -12219.0371\n",
      "Epoch [7/200] Batch 140/469                   Loss D: -1007.7014, loss G: -12643.5254\n",
      "Epoch [7/200] Batch 160/469                   Loss D: 12.9843, loss G: -12145.7666\n",
      "Epoch [7/200] Batch 180/469                   Loss D: -13.9949, loss G: -12200.9199\n",
      "Epoch [7/200] Batch 200/469                   Loss D: -8.3041, loss G: -12181.6973\n",
      "Epoch [7/200] Batch 220/469                   Loss D: 99.0417, loss G: -12325.6914\n",
      "Epoch [7/200] Batch 240/469                   Loss D: -401.7379, loss G: -11900.7139\n",
      "Epoch [7/200] Batch 260/469                   Loss D: -64.7374, loss G: -12507.9502\n",
      "Epoch [7/200] Batch 280/469                   Loss D: 73.4219, loss G: -11914.2070\n",
      "Epoch [7/200] Batch 300/469                   Loss D: -722.6551, loss G: -11971.4346\n",
      "Epoch [7/200] Batch 320/469                   Loss D: -149.3011, loss G: -11964.9619\n",
      "Epoch [7/200] Batch 340/469                   Loss D: 92.4067, loss G: -11759.7715\n",
      "Epoch [7/200] Batch 360/469                   Loss D: 2.7304, loss G: -11959.9277\n",
      "Epoch [7/200] Batch 380/469                   Loss D: 289.8974, loss G: -11881.2109\n",
      "Epoch [7/200] Batch 400/469                   Loss D: -1004.4594, loss G: -12313.3877\n",
      "Epoch [7/200] Batch 420/469                   Loss D: -516.0156, loss G: -11772.7705\n",
      "Epoch [7/200] Batch 440/469                   Loss D: -5.3181, loss G: -12052.0859\n",
      "Epoch [7/200] Batch 460/469                   Loss D: -938.7128, loss G: -12271.0137\n",
      "Epoch [8/200] Batch 20/469                   Loss D: -754.6145, loss G: -11628.4590\n",
      "Epoch [8/200] Batch 40/469                   Loss D: 4.7543, loss G: -11735.3057\n",
      "Epoch [8/200] Batch 60/469                   Loss D: -503.5099, loss G: -11927.9053\n",
      "Epoch [8/200] Batch 80/469                   Loss D: 3714.2007, loss G: -11791.1299\n",
      "Epoch [8/200] Batch 100/469                   Loss D: 26.3122, loss G: -11679.3252\n",
      "Epoch [8/200] Batch 120/469                   Loss D: 11.5880, loss G: -11674.0479\n",
      "Epoch [8/200] Batch 140/469                   Loss D: -574.6565, loss G: -11550.9170\n",
      "Epoch [8/200] Batch 160/469                   Loss D: -1235.2245, loss G: -11899.8311\n",
      "Epoch [8/200] Batch 180/469                   Loss D: 29.5442, loss G: -11883.7041\n",
      "Epoch [8/200] Batch 200/469                   Loss D: -517.0409, loss G: -11606.4316\n",
      "Epoch [8/200] Batch 220/469                   Loss D: -1486.4238, loss G: -10790.4541\n",
      "Epoch [8/200] Batch 240/469                   Loss D: 21.5070, loss G: -11534.5176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200] Batch 260/469                   Loss D: 18.1340, loss G: -11746.9141\n",
      "Epoch [8/200] Batch 280/469                   Loss D: -1002.8507, loss G: -10625.9551\n",
      "Epoch [8/200] Batch 300/469                   Loss D: -407.1713, loss G: -11007.4668\n",
      "Epoch [8/200] Batch 320/469                   Loss D: -1991.2924, loss G: -11287.3311\n",
      "Epoch [8/200] Batch 340/469                   Loss D: -2254.4614, loss G: -10988.5908\n",
      "Epoch [8/200] Batch 360/469                   Loss D: -2693.8308, loss G: -10624.8066\n",
      "Epoch [8/200] Batch 380/469                   Loss D: -2994.8428, loss G: -10017.7393\n",
      "Epoch [8/200] Batch 400/469                   Loss D: -3771.5732, loss G: -9205.4668\n",
      "Epoch [8/200] Batch 420/469                   Loss D: -3946.7166, loss G: -8305.1426\n",
      "Epoch [8/200] Batch 440/469                   Loss D: -6206.9893, loss G: -6812.7900\n",
      "Epoch [8/200] Batch 460/469                   Loss D: -7504.1318, loss G: -4617.1504\n",
      "Epoch [9/200] Batch 20/469                   Loss D: -10247.3711, loss G: -1531.7396\n",
      "Epoch [9/200] Batch 40/469                   Loss D: -12377.4561, loss G: -57.7797\n",
      "Epoch [9/200] Batch 60/469                   Loss D: -13728.7832, loss G: 1072.6698\n",
      "Epoch [9/200] Batch 80/469                   Loss D: -10849.4854, loss G: 1998.7532\n",
      "Epoch [9/200] Batch 100/469                   Loss D: -16480.0684, loss G: 3347.6970\n",
      "Epoch [9/200] Batch 120/469                   Loss D: -17121.3359, loss G: 4477.1323\n",
      "Epoch [9/200] Batch 140/469                   Loss D: -19094.7754, loss G: 5704.8701\n",
      "Epoch [9/200] Batch 160/469                   Loss D: -18677.8047, loss G: 6663.5107\n",
      "Epoch [9/200] Batch 180/469                   Loss D: -19874.0664, loss G: 7754.2944\n",
      "Epoch [9/200] Batch 200/469                   Loss D: -23379.6582, loss G: 9100.0879\n",
      "Epoch [9/200] Batch 220/469                   Loss D: -23742.1328, loss G: 10033.6143\n",
      "Epoch [9/200] Batch 240/469                   Loss D: -25536.2344, loss G: 10735.9609\n",
      "Epoch [9/200] Batch 260/469                   Loss D: -25826.6289, loss G: 11505.7363\n",
      "Epoch [9/200] Batch 280/469                   Loss D: -26723.8184, loss G: 11876.4668\n",
      "Epoch [9/200] Batch 300/469                   Loss D: -22192.2266, loss G: 11393.4531\n",
      "Epoch [9/200] Batch 320/469                   Loss D: -15828.4043, loss G: 10286.3281\n",
      "Epoch [9/200] Batch 340/469                   Loss D: -20069.8145, loss G: 6483.3257\n",
      "Epoch [9/200] Batch 360/469                   Loss D: -18665.4395, loss G: 11518.4463\n",
      "Epoch [9/200] Batch 380/469                   Loss D: -11906.1504, loss G: 11119.9844\n",
      "Epoch [9/200] Batch 400/469                   Loss D: -9838.9170, loss G: 11066.1309\n",
      "Epoch [9/200] Batch 420/469                   Loss D: -15079.2061, loss G: 11007.0703\n",
      "Epoch [9/200] Batch 440/469                   Loss D: -13919.9346, loss G: 6417.6006\n",
      "Epoch [9/200] Batch 460/469                   Loss D: -8180.6714, loss G: 4819.0410\n",
      "Epoch [10/200] Batch 20/469                   Loss D: -4308.9341, loss G: 9092.6650\n",
      "Epoch [10/200] Batch 40/469                   Loss D: -11514.6953, loss G: 12374.3262\n",
      "Epoch [10/200] Batch 60/469                   Loss D: -3534.6467, loss G: 10093.0586\n",
      "Epoch [10/200] Batch 80/469                   Loss D: -17898.4609, loss G: 12522.5508\n",
      "Epoch [10/200] Batch 100/469                   Loss D: -11929.0332, loss G: 12949.7549\n",
      "Epoch [10/200] Batch 120/469                   Loss D: -21206.3281, loss G: 6400.9233\n",
      "Epoch [10/200] Batch 140/469                   Loss D: -18001.7051, loss G: 13322.3545\n",
      "Epoch [10/200] Batch 160/469                   Loss D: -13341.9727, loss G: 14011.6309\n",
      "Epoch [10/200] Batch 180/469                   Loss D: -15466.9004, loss G: 350.1385\n",
      "Epoch [10/200] Batch 200/469                   Loss D: -17942.4316, loss G: 13535.3066\n",
      "Epoch [10/200] Batch 220/469                   Loss D: -11601.7217, loss G: 13432.9346\n",
      "Epoch [10/200] Batch 240/469                   Loss D: -22109.1738, loss G: 13573.3711\n",
      "Epoch [10/200] Batch 260/469                   Loss D: 2149.3115, loss G: 9924.7900\n",
      "Epoch [10/200] Batch 280/469                   Loss D: -23720.2520, loss G: 6486.1021\n",
      "Epoch [10/200] Batch 300/469                   Loss D: -17569.8145, loss G: 14872.0996\n",
      "Epoch [10/200] Batch 320/469                   Loss D: -18854.7656, loss G: -1721.6765\n",
      "Epoch [10/200] Batch 340/469                   Loss D: -28283.4473, loss G: 12917.3281\n",
      "Epoch [10/200] Batch 360/469                   Loss D: -26631.0977, loss G: 13069.5859\n",
      "Epoch [10/200] Batch 380/469                   Loss D: -10007.9531, loss G: 5741.4639\n",
      "Epoch [10/200] Batch 400/469                   Loss D: -28675.4492, loss G: 13296.4561\n",
      "Epoch [10/200] Batch 420/469                   Loss D: -32162.7617, loss G: 15126.3223\n",
      "Epoch [10/200] Batch 440/469                   Loss D: -20653.2598, loss G: 108.8311\n",
      "Epoch [10/200] Batch 460/469                   Loss D: -27800.8887, loss G: 13266.1719\n",
      "Epoch [11/200] Batch 20/469                   Loss D: -28965.7207, loss G: 11665.5537\n",
      "Epoch [11/200] Batch 40/469                   Loss D: -8244.5039, loss G: 11565.3320\n",
      "Epoch [11/200] Batch 60/469                   Loss D: -20316.6465, loss G: 15823.5869\n",
      "Epoch [11/200] Batch 80/469                   Loss D: -33606.4414, loss G: 16437.8984\n",
      "Epoch [11/200] Batch 100/469                   Loss D: -32760.8496, loss G: 15117.0156\n",
      "Epoch [11/200] Batch 120/469                   Loss D: -5378.5269, loss G: 13713.3896\n",
      "Epoch [11/200] Batch 140/469                   Loss D: -22253.9746, loss G: 16529.0391\n",
      "Epoch [11/200] Batch 160/469                   Loss D: -32581.5195, loss G: 15021.8477\n",
      "Epoch [11/200] Batch 180/469                   Loss D: -31398.4570, loss G: 12442.4590\n",
      "Epoch [11/200] Batch 200/469                   Loss D: -34503.4883, loss G: 16117.1133\n",
      "Epoch [11/200] Batch 220/469                   Loss D: -12888.0000, loss G: 17050.7480\n",
      "Epoch [11/200] Batch 240/469                   Loss D: -30076.1426, loss G: 8840.9863\n",
      "Epoch [11/200] Batch 260/469                   Loss D: -33737.7578, loss G: 15149.1855\n",
      "Epoch [11/200] Batch 280/469                   Loss D: -34975.8633, loss G: 16829.0000\n",
      "Epoch [11/200] Batch 300/469                   Loss D: -32962.0938, loss G: 13958.8477\n",
      "Epoch [11/200] Batch 320/469                   Loss D: -11140.5801, loss G: 7276.9307\n",
      "Epoch [11/200] Batch 340/469                   Loss D: -30741.8809, loss G: 19189.8242\n",
      "Epoch [11/200] Batch 360/469                   Loss D: -36344.7734, loss G: 14617.2441\n",
      "Epoch [11/200] Batch 380/469                   Loss D: -33562.6211, loss G: 5345.8350\n",
      "Epoch [11/200] Batch 400/469                   Loss D: -31431.0684, loss G: 18799.9746\n",
      "Epoch [11/200] Batch 420/469                   Loss D: -36804.2070, loss G: 18975.2109\n",
      "Epoch [11/200] Batch 440/469                   Loss D: -33427.9570, loss G: 14794.0840\n",
      "Epoch [11/200] Batch 460/469                   Loss D: -35227.5781, loss G: 14516.4824\n",
      "Epoch [12/200] Batch 20/469                   Loss D: -36973.2891, loss G: 18317.6172\n",
      "Epoch [12/200] Batch 40/469                   Loss D: -606.7712, loss G: 20664.2266\n",
      "Epoch [12/200] Batch 60/469                   Loss D: -40258.1094, loss G: 16512.1270\n",
      "Epoch [12/200] Batch 80/469                   Loss D: -39473.5938, loss G: 18992.4023\n",
      "Epoch [12/200] Batch 100/469                   Loss D: -36478.4141, loss G: 14824.6738\n",
      "Epoch [12/200] Batch 120/469                   Loss D: -33220.1172, loss G: 15167.5020\n",
      "Epoch [12/200] Batch 140/469                   Loss D: -39507.4258, loss G: 19152.6621\n",
      "Epoch [12/200] Batch 160/469                   Loss D: -39243.2930, loss G: 20297.2930\n",
      "Epoch [12/200] Batch 180/469                   Loss D: -41387.3672, loss G: 19688.0996\n",
      "Epoch [12/200] Batch 200/469                   Loss D: -41251.3438, loss G: 19939.1875\n",
      "Epoch [12/200] Batch 220/469                   Loss D: -20023.5020, loss G: 3237.3169\n",
      "Epoch [12/200] Batch 240/469                   Loss D: -38836.3750, loss G: 21402.7227\n",
      "Epoch [12/200] Batch 260/469                   Loss D: -42406.3281, loss G: 20382.8242\n",
      "Epoch [12/200] Batch 280/469                   Loss D: -40113.4180, loss G: 21946.2109\n",
      "Epoch [12/200] Batch 300/469                   Loss D: -43000.3438, loss G: 20233.3633\n",
      "Epoch [12/200] Batch 320/469                   Loss D: -18895.1348, loss G: 7225.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/200] Batch 340/469                   Loss D: -41809.2344, loss G: 14663.4619\n",
      "Epoch [12/200] Batch 360/469                   Loss D: -31873.0293, loss G: 23603.8984\n",
      "Epoch [12/200] Batch 380/469                   Loss D: -41971.7539, loss G: 21719.4492\n",
      "Epoch [12/200] Batch 400/469                   Loss D: -12061.2188, loss G: 20668.6289\n",
      "Epoch [12/200] Batch 420/469                   Loss D: -45223.6172, loss G: 22448.9961\n",
      "Epoch [12/200] Batch 440/469                   Loss D: -23649.7637, loss G: 8010.6260\n",
      "Epoch [12/200] Batch 460/469                   Loss D: -42448.3320, loss G: 24100.4297\n",
      "Epoch [13/200] Batch 20/469                   Loss D: -34207.4766, loss G: 24353.4531\n",
      "Epoch [13/200] Batch 40/469                   Loss D: -44271.9688, loss G: 22902.3105\n",
      "Epoch [13/200] Batch 60/469                   Loss D: -44089.9609, loss G: 18973.0273\n",
      "Epoch [13/200] Batch 80/469                   Loss D: -47006.6055, loss G: 22118.6797\n",
      "Epoch [13/200] Batch 100/469                   Loss D: -40181.0898, loss G: 18948.8945\n",
      "Epoch [13/200] Batch 120/469                   Loss D: -10683.1621, loss G: 19655.8594\n",
      "Epoch [13/200] Batch 140/469                   Loss D: -38405.2734, loss G: 25249.5059\n",
      "Epoch [13/200] Batch 160/469                   Loss D: -48343.9570, loss G: 23764.1719\n",
      "Epoch [13/200] Batch 180/469                   Loss D: -51415.0664, loss G: 24355.1992\n",
      "Epoch [13/200] Batch 200/469                   Loss D: -33837.0625, loss G: -9376.1504\n",
      "Epoch [13/200] Batch 220/469                   Loss D: -43563.5117, loss G: 20839.5586\n",
      "Epoch [13/200] Batch 240/469                   Loss D: -43621.2500, loss G: 4428.4521\n",
      "Epoch [13/200] Batch 260/469                   Loss D: -51019.7344, loss G: 24832.4922\n",
      "Epoch [13/200] Batch 280/469                   Loss D: -50163.7383, loss G: 20614.8047\n",
      "Epoch [13/200] Batch 300/469                   Loss D: -52555.7148, loss G: 24918.4492\n",
      "Epoch [13/200] Batch 320/469                   Loss D: -44366.0938, loss G: 11480.5840\n",
      "Epoch [13/200] Batch 340/469                   Loss D: -18547.0781, loss G: 19824.7812\n",
      "Epoch [13/200] Batch 360/469                   Loss D: -36636.4258, loss G: 27408.6367\n",
      "Epoch [13/200] Batch 380/469                   Loss D: -52740.9805, loss G: 25784.7305\n",
      "Epoch [13/200] Batch 400/469                   Loss D: -27519.0156, loss G: 28167.1133\n",
      "Epoch [13/200] Batch 420/469                   Loss D: -51767.1953, loss G: 24854.2383\n",
      "Epoch [13/200] Batch 440/469                   Loss D: -60483.3633, loss G: 28933.3105\n",
      "Epoch [13/200] Batch 460/469                   Loss D: -31904.2695, loss G: 5525.8765\n",
      "Epoch [14/200] Batch 20/469                   Loss D: -30810.9883, loss G: 28802.7578\n",
      "Epoch [14/200] Batch 40/469                   Loss D: -53922.8008, loss G: 24792.0938\n",
      "Epoch [14/200] Batch 60/469                   Loss D: -55970.6953, loss G: 26503.8594\n",
      "Epoch [14/200] Batch 80/469                   Loss D: -62409.4414, loss G: 29749.0234\n",
      "Epoch [14/200] Batch 100/469                   Loss D: -51316.7891, loss G: 26947.1895\n",
      "Epoch [14/200] Batch 120/469                   Loss D: -53286.9062, loss G: 30051.3242\n",
      "Epoch [14/200] Batch 140/469                   Loss D: -61934.5273, loss G: 29780.8945\n",
      "Epoch [14/200] Batch 160/469                   Loss D: -50610.9102, loss G: 29939.4277\n",
      "Epoch [14/200] Batch 180/469                   Loss D: -53100.4844, loss G: 19377.4941\n",
      "Epoch [14/200] Batch 200/469                   Loss D: -56734.6719, loss G: 27798.0371\n",
      "Epoch [14/200] Batch 220/469                   Loss D: -18626.4004, loss G: 8251.0566\n",
      "Epoch [14/200] Batch 240/469                   Loss D: -24729.5547, loss G: 6362.5288\n",
      "Epoch [14/200] Batch 260/469                   Loss D: -57787.1992, loss G: 29721.8809\n",
      "Epoch [14/200] Batch 280/469                   Loss D: -52887.5703, loss G: 27226.4609\n",
      "Epoch [14/200] Batch 300/469                   Loss D: -35218.6367, loss G: 17614.6914\n",
      "Epoch [14/200] Batch 320/469                   Loss D: -61599.4648, loss G: 31064.0098\n",
      "Epoch [14/200] Batch 340/469                   Loss D: -53711.8516, loss G: 23986.2344\n",
      "Epoch [14/200] Batch 360/469                   Loss D: -61460.2812, loss G: 29250.7266\n",
      "Epoch [14/200] Batch 380/469                   Loss D: -61906.9570, loss G: 29025.8359\n",
      "Epoch [14/200] Batch 400/469                   Loss D: -66136.3203, loss G: 32495.6426\n",
      "Epoch [14/200] Batch 420/469                   Loss D: -33900.1133, loss G: 32153.5391\n",
      "Epoch [14/200] Batch 440/469                   Loss D: -50505.0156, loss G: 30814.8359\n",
      "Epoch [14/200] Batch 460/469                   Loss D: -46189.8086, loss G: 33703.5195\n",
      "Epoch [15/200] Batch 20/469                   Loss D: -14870.2051, loss G: 32721.4688\n",
      "Epoch [15/200] Batch 40/469                   Loss D: -58470.5156, loss G: 32835.0742\n",
      "Epoch [15/200] Batch 60/469                   Loss D: -68814.6562, loss G: 33834.6680\n",
      "Epoch [15/200] Batch 80/469                   Loss D: -5184.9902, loss G: 32690.3359\n",
      "Epoch [15/200] Batch 100/469                   Loss D: -70381.6641, loss G: 34025.6953\n",
      "Epoch [15/200] Batch 120/469                   Loss D: -49224.2227, loss G: 33305.9688\n",
      "Epoch [15/200] Batch 140/469                   Loss D: -60472.8711, loss G: 28371.2969\n",
      "Epoch [15/200] Batch 160/469                   Loss D: -61959.6758, loss G: 31234.1543\n",
      "Epoch [15/200] Batch 180/469                   Loss D: 893.0734, loss G: 35762.1562\n",
      "Epoch [15/200] Batch 200/469                   Loss D: -58019.0234, loss G: 31680.8438\n",
      "Epoch [15/200] Batch 220/469                   Loss D: -50321.4883, loss G: 33808.6562\n",
      "Epoch [15/200] Batch 240/469                   Loss D: -63643.5039, loss G: 30173.8867\n",
      "Epoch [15/200] Batch 260/469                   Loss D: -64766.2773, loss G: 32264.8203\n",
      "Epoch [15/200] Batch 280/469                   Loss D: -57993.9688, loss G: 12447.2998\n",
      "Epoch [15/200] Batch 300/469                   Loss D: -67679.2734, loss G: 31336.4727\n",
      "Epoch [15/200] Batch 320/469                   Loss D: -65039.3789, loss G: 31064.6270\n",
      "Epoch [15/200] Batch 340/469                   Loss D: -67489.0312, loss G: 32006.1406\n",
      "Epoch [15/200] Batch 360/469                   Loss D: -43047.3047, loss G: 6544.0762\n",
      "Epoch [15/200] Batch 380/469                   Loss D: -64372.5781, loss G: 30332.5918\n",
      "Epoch [15/200] Batch 400/469                   Loss D: -68631.4531, loss G: 31888.8691\n",
      "Epoch [15/200] Batch 420/469                   Loss D: -43321.7773, loss G: 16910.2793\n",
      "Epoch [15/200] Batch 440/469                   Loss D: -34933.1602, loss G: 11665.3867\n",
      "Epoch [15/200] Batch 460/469                   Loss D: 19706.9922, loss G: 34262.6211\n",
      "Epoch [16/200] Batch 20/469                   Loss D: -30279.5020, loss G: 15806.4131\n",
      "Epoch [16/200] Batch 40/469                   Loss D: 151.7668, loss G: 38848.8711\n",
      "Epoch [16/200] Batch 60/469                   Loss D: -72380.8125, loss G: 35599.0625\n",
      "Epoch [16/200] Batch 80/469                   Loss D: -72121.8047, loss G: 34530.5703\n",
      "Epoch [16/200] Batch 100/469                   Loss D: 272.1322, loss G: 39425.7266\n",
      "Epoch [16/200] Batch 120/469                   Loss D: -70083.1094, loss G: 32365.6172\n",
      "Epoch [16/200] Batch 140/469                   Loss D: -44432.7500, loss G: 38040.5000\n",
      "Epoch [16/200] Batch 160/469                   Loss D: -70989.8516, loss G: 27784.7617\n",
      "Epoch [16/200] Batch 180/469                   Loss D: -72697.7891, loss G: 34435.1406\n",
      "Epoch [16/200] Batch 200/469                   Loss D: -51748.0469, loss G: 40019.6406\n",
      "Epoch [16/200] Batch 220/469                   Loss D: -73965.6484, loss G: 35045.4531\n",
      "Epoch [16/200] Batch 240/469                   Loss D: -14445.5156, loss G: 21045.6484\n",
      "Epoch [16/200] Batch 260/469                   Loss D: -73586.2422, loss G: 34728.6250\n",
      "Epoch [16/200] Batch 280/469                   Loss D: -74251.0781, loss G: 35449.1328\n",
      "Epoch [16/200] Batch 300/469                   Loss D: -83016.4531, loss G: 40019.3359\n",
      "Epoch [16/200] Batch 320/469                   Loss D: -68517.5000, loss G: 38101.7109\n",
      "Epoch [16/200] Batch 340/469                   Loss D: -65383.9375, loss G: 26218.0000\n",
      "Epoch [16/200] Batch 360/469                   Loss D: -79109.7109, loss G: 37171.3984\n",
      "Epoch [16/200] Batch 380/469                   Loss D: -74671.7656, loss G: 34907.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/200] Batch 400/469                   Loss D: -80530.2578, loss G: 41550.2422\n",
      "Epoch [16/200] Batch 420/469                   Loss D: -86481.3125, loss G: 41684.8711\n",
      "Epoch [16/200] Batch 440/469                   Loss D: -80510.3672, loss G: 38657.3906\n",
      "Epoch [16/200] Batch 460/469                   Loss D: -76582.9922, loss G: 36279.1758\n",
      "Epoch [17/200] Batch 20/469                   Loss D: -40075.4766, loss G: 15179.3438\n",
      "Epoch [17/200] Batch 40/469                   Loss D: -75684.0547, loss G: 36220.4531\n",
      "Epoch [17/200] Batch 60/469                   Loss D: -55419.9375, loss G: 14853.4883\n",
      "Epoch [17/200] Batch 80/469                   Loss D: -45827.9766, loss G: 41279.5312\n",
      "Epoch [17/200] Batch 100/469                   Loss D: -73226.6641, loss G: 38152.8203\n",
      "Epoch [17/200] Batch 120/469                   Loss D: -71032.9375, loss G: 42564.9492\n",
      "Epoch [17/200] Batch 140/469                   Loss D: -81583.6328, loss G: 37505.6250\n",
      "Epoch [17/200] Batch 160/469                   Loss D: -71270.3125, loss G: 39037.4219\n",
      "Epoch [17/200] Batch 180/469                   Loss D: -72876.2969, loss G: 32568.0781\n",
      "Epoch [17/200] Batch 200/469                   Loss D: -17957.4609, loss G: 25797.0098\n",
      "Epoch [17/200] Batch 220/469                   Loss D: -11660.9443, loss G: 33126.6016\n",
      "Epoch [17/200] Batch 240/469                   Loss D: -84484.7812, loss G: 42059.2969\n",
      "Epoch [17/200] Batch 260/469                   Loss D: -50004.4531, loss G: 42947.7773\n",
      "Epoch [17/200] Batch 280/469                   Loss D: -84382.5156, loss G: 40947.1992\n",
      "Epoch [17/200] Batch 300/469                   Loss D: -22712.3379, loss G: 1717.4734\n",
      "Epoch [17/200] Batch 320/469                   Loss D: -89570.6719, loss G: 42651.9844\n",
      "Epoch [17/200] Batch 340/469                   Loss D: -93015.2109, loss G: 43825.1250\n",
      "Epoch [17/200] Batch 360/469                   Loss D: -87557.1250, loss G: 41865.6953\n",
      "Epoch [17/200] Batch 380/469                   Loss D: -87368.4766, loss G: 40601.6133\n",
      "Epoch [17/200] Batch 400/469                   Loss D: -87417.3438, loss G: 43010.9766\n",
      "Epoch [17/200] Batch 420/469                   Loss D: -37645.6016, loss G: 41397.8594\n",
      "Epoch [17/200] Batch 440/469                   Loss D: -70899.1875, loss G: 45497.3828\n",
      "Epoch [17/200] Batch 460/469                   Loss D: -95454.6797, loss G: 32930.7109\n",
      "Epoch [18/200] Batch 20/469                   Loss D: -84250.1797, loss G: 38248.8984\n",
      "Epoch [18/200] Batch 40/469                   Loss D: -101158.0703, loss G: 48137.0859\n",
      "Epoch [18/200] Batch 60/469                   Loss D: -68302.1875, loss G: 46268.7578\n",
      "Epoch [18/200] Batch 80/469                   Loss D: -101559.6406, loss G: 48032.9844\n",
      "Epoch [18/200] Batch 100/469                   Loss D: -87140.5781, loss G: 40515.1953\n",
      "Epoch [18/200] Batch 120/469                   Loss D: -1208.0281, loss G: 47605.2344\n",
      "Epoch [18/200] Batch 140/469                   Loss D: -104348.9062, loss G: 49289.8984\n",
      "Epoch [18/200] Batch 160/469                   Loss D: 72.2600, loss G: 49472.8867\n",
      "Epoch [18/200] Batch 180/469                   Loss D: -82669.7969, loss G: 41402.5859\n",
      "Epoch [18/200] Batch 200/469                   Loss D: -28970.3164, loss G: 43335.9180\n",
      "Epoch [18/200] Batch 220/469                   Loss D: -79985.0859, loss G: 45317.3516\n",
      "Epoch [18/200] Batch 240/469                   Loss D: -102717.2578, loss G: 49959.2656\n",
      "Epoch [18/200] Batch 260/469                   Loss D: -89742.8984, loss G: 42969.3125\n",
      "Epoch [18/200] Batch 280/469                   Loss D: -78063.0547, loss G: 47170.2188\n",
      "Epoch [18/200] Batch 300/469                   Loss D: -71765.1641, loss G: 48445.4062\n",
      "Epoch [18/200] Batch 320/469                   Loss D: -93420.6328, loss G: -2012.1970\n",
      "Epoch [18/200] Batch 340/469                   Loss D: -9318.2637, loss G: 22425.7422\n",
      "Epoch [18/200] Batch 360/469                   Loss D: -91771.1797, loss G: 44638.7695\n",
      "Epoch [18/200] Batch 380/469                   Loss D: -91110.8828, loss G: 45033.1367\n",
      "Epoch [18/200] Batch 400/469                   Loss D: -69751.9297, loss G: 50822.0547\n",
      "Epoch [18/200] Batch 420/469                   Loss D: -89361.1875, loss G: 33308.2070\n",
      "Epoch [18/200] Batch 440/469                   Loss D: -46759.7422, loss G: 51536.2656\n",
      "Epoch [18/200] Batch 460/469                   Loss D: 2027.8875, loss G: 52808.7461\n",
      "Epoch [19/200] Batch 20/469                   Loss D: -94058.5234, loss G: 46070.7969\n",
      "Epoch [19/200] Batch 40/469                   Loss D: -110196.3672, loss G: 52012.0156\n",
      "Epoch [19/200] Batch 60/469                   Loss D: -83845.2422, loss G: 48224.6953\n",
      "Epoch [19/200] Batch 80/469                   Loss D: -97927.8125, loss G: 45240.2578\n",
      "Epoch [19/200] Batch 100/469                   Loss D: -90423.3516, loss G: 46607.5078\n",
      "Epoch [19/200] Batch 120/469                   Loss D: -92845.8047, loss G: 47395.1953\n",
      "Epoch [19/200] Batch 140/469                   Loss D: -93600.6172, loss G: 49192.0938\n",
      "Epoch [19/200] Batch 160/469                   Loss D: -99212.5391, loss G: 48677.6562\n",
      "Epoch [19/200] Batch 180/469                   Loss D: -110106.0234, loss G: 53565.8477\n",
      "Epoch [19/200] Batch 200/469                   Loss D: -8081.5781, loss G: 44709.1875\n",
      "Epoch [19/200] Batch 220/469                   Loss D: -103900.4375, loss G: 48716.7656\n",
      "Epoch [19/200] Batch 240/469                   Loss D: -44768.5547, loss G: 51622.1797\n",
      "Epoch [19/200] Batch 260/469                   Loss D: -103738.8828, loss G: 48195.1367\n",
      "Epoch [19/200] Batch 280/469                   Loss D: -113008.3047, loss G: 54347.5859\n",
      "Epoch [19/200] Batch 300/469                   Loss D: -101880.6484, loss G: 49164.0156\n",
      "Epoch [19/200] Batch 320/469                   Loss D: -96953.9453, loss G: 49585.8867\n",
      "Epoch [19/200] Batch 340/469                   Loss D: -1353.1479, loss G: 53210.2031\n",
      "Epoch [19/200] Batch 360/469                   Loss D: -104589.2578, loss G: 48223.1680\n",
      "Epoch [19/200] Batch 380/469                   Loss D: -55538.7109, loss G: 52898.1250\n",
      "Epoch [19/200] Batch 400/469                   Loss D: -109990.9375, loss G: 51689.7539\n",
      "Epoch [19/200] Batch 420/469                   Loss D: -85612.7188, loss G: 55663.3047\n",
      "Epoch [19/200] Batch 440/469                   Loss D: -106264.2031, loss G: 50185.7109\n",
      "Epoch [19/200] Batch 460/469                   Loss D: -116443.6797, loss G: 50981.4180\n",
      "Epoch [20/200] Batch 20/469                   Loss D: -105791.7188, loss G: 51452.6406\n",
      "Epoch [20/200] Batch 40/469                   Loss D: -102475.5625, loss G: 51493.7227\n",
      "Epoch [20/200] Batch 60/469                   Loss D: -112794.7188, loss G: 54337.1016\n",
      "Epoch [20/200] Batch 80/469                   Loss D: -108372.4844, loss G: 51159.2812\n",
      "Epoch [20/200] Batch 100/469                   Loss D: -15138.7705, loss G: 38279.9648\n",
      "Epoch [20/200] Batch 120/469                   Loss D: -122397.3750, loss G: 57404.2695\n",
      "Epoch [20/200] Batch 140/469                   Loss D: -114255.9766, loss G: 59212.4453\n",
      "Epoch [20/200] Batch 160/469                   Loss D: -1510.2419, loss G: 57816.5000\n",
      "Epoch [20/200] Batch 180/469                   Loss D: -103364.0234, loss G: 54147.4375\n",
      "Epoch [20/200] Batch 200/469                   Loss D: -2560.5696, loss G: 57395.5703\n"
     ]
    }
   ],
   "source": [
    "LAMBDA_GP = 10 \n",
    "gen.train()\n",
    "disc.train()\n",
    "step = 0 \n",
    "for epoch in range(epochs): \n",
    "    for batch_idx, (true_imgs, y_labels) in enumerate(loader):\n",
    "        \n",
    "        true_imgs = true_imgs.to(device)\n",
    "        cur_batch_size = true_imgs.shape[0]\n",
    "        \n",
    "        # update the critic \n",
    "        \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, z_dim,1,1).to(device)\n",
    "            labels_onehot = F.one_hot(y_labels, nlabels).view(cur_batch_size, -1, 1,1).to(device)\n",
    "            noise_embed_label = torch.cat([noise, labels_onehot], axis = 1)\n",
    "            \n",
    "            imgs_onehot = torch.ones(size = (cur_batch_size, nlabels,img_size, img_size), dtype = true_imgs.dtype, device = device)\n",
    "            img_labels = imgs_onehot * labels_onehot \n",
    "            true_img_nd_labels = torch.cat([true_imgs, img_labels], axis = 1)\n",
    "            \n",
    "            # make the prediction with critic and generator \n",
    "            \n",
    "            #print(true_img_nd_labels.shape)\n",
    "            \n",
    "            fake = gen(noise_embed_label)\n",
    "            #raise Exception\n",
    "            fake_img_nd_labels = torch.cat([fake, img_labels], axis =1)\n",
    "            critic_real = disc(true_img_nd_labels).reshape(-1)\n",
    "            critic_fake = disc(fake_img_nd_labels).reshape(-1)\n",
    "            \n",
    "            gp = gradient_penalty(disc, true_imgs, fake, img_labels, device = device)\n",
    "            \n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            \n",
    "            disc.zero_grad()\n",
    "            loss_critic.backward(retain_graph = True)\n",
    "            disc_opt.step()\n",
    "            #print(\"hi\")\n",
    "            \n",
    "        # update the generator \n",
    "        critic_fake = disc(fake_img_nd_labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(critic_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        \n",
    "        gen_opt.step()\n",
    "        \n",
    "        #print('Done step {}'.format(step))\n",
    "        \n",
    "        #Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 20 == 0 and batch_idx > 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(true_imgs[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32,:,:,:], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "        step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
